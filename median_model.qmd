---
title: "median_model"
format: html
editor: visual
---

## load libraries

```{r libraries}
library(data.table)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(explore)
library(vip)
```

## path to files

```{r path-to-files}
# path_files <- "X:/PublicData/QC/HF/2022/02/txt/"
path_files <- "X:/PublicData/QC/temporary_all_HF/QC_comb_txt_results/"
```


## load files

```{r files}
# summary <-
#   data.table::fread(input = paste0(path_files, "summary.txt")) %>% 
#   tibble::as_tibble(.name_repair = janitor::make_clean_names) %>% 
#   dplyr::select(
#     raw_file,
#     ms_ms,
#     ms_ms_submitted,
#     ms_ms_identified,
#     ms_ms_identified_percent,
#     peptide_sequences_identified,
#     peaks_sequenced_percent,
#     peaks_repeatedly_sequenced_percent
#   ) %>% 
#   dplyr::filter(!raw_file == "Total")
# 
# evidence <-
#   data.table::fread(input = paste0(path_files, "evidence.txt")) %>%
#   tibble::as_tibble(.name_repair = janitor::make_clean_names) %>% 
#   dplyr::filter(!reverse == "+" & !potential_contaminant == "+") %>%
#   dplyr::select(
#     raw_file,
#     experiment,
#     charge,
#     m_z,
#     mass,
#     score,
#     uncalibrated_calibrated_m_z_ppm,
#     mass_error_ppm,
#     uncalibrated_mass_error_ppm,
#     max_intensity_m_z_0,
#     retention_time,
#     retention_length,
#     number_of_data_points,
#     number_of_scans,
#     ms_ms_count,
#     intensity
#   ) 
# 
# protein_groups <-
#   data.table::fread(input = paste0(path_files, "proteinGroups.txt")) %>%
#   tibble::as_tibble(.name_repair = janitor::make_clean_names) %>%
#   dplyr::filter(!reverse == "+" & !potential_contaminant == "+" & !only_identified_by_site == "+") %>% 
#   dplyr::select(starts_with("peptides_"))

# all_peptides <-
#   data.table::fread(input = paste0(path_files, "allPeptides.txt")) %>%
#   tibble::as_tibble(.name_repair = janitor::make_clean_names) %>%
#   dplyr::select(
#     raw_file,
#     mass_precision_ppm,
#     retention_time,
#     retention_length,
#     retention_length_fwhm
#     # score
#     # msms_isotope_indices
#   )

# ms_scan <-
#   data.table::fread(input = paste0(path_files, "msScans.txt")) %>%
#   tibble::as_tibble(.name_repair = janitor::make_clean_names) %>% 
#   dplyr::select(
#     raw_file,
#     retention_time,
#     cycle_time,
#     ion_injection_time,
#     total_ion_current,
#     base_peak_intensity,
#     peak_length,
#     identified_multiplets_s,
#     ms_ms_s,
#     identified_ms_ms_s,
#     ms_ms_identification_rate_percent,
#     intens_comp_factor,
#     ctcd_comp,
#     raw_ov_ft_t,
#     agc_fill
#   ) 
# 
# msms_scan <-
#   data.table::fread(input = paste0(path_files, "msmsScans.txt")) %>% 
#   tibble::as_tibble(.name_repair = janitor::make_clean_names) %>%
#   dplyr::select(
#     raw_file,
#     retention_time,
#     ion_injection_time,
#     total_ion_current,
#     collision_energy,
#     base_peak_intensity,
#     elapsed_time,
#     identified,
#     precursor_intensity,
#     score
#   ) 

# msms <-
#   data.table::fread(input = paste0(path_files, "msms.txt")) %>%
#   tibble::as_tibble(.name_repair = janitor::make_clean_names) %>% 
#   dplyr::select(
#     raw_file, 
#     isotope_index,
#     mass_error_ppm,
#     simple_mass_error_ppm,
#     precursor_intensity,
#     retention_time,
#     score,
#     mass_deviations_ppm,
#     number_of_matches,
#     intensity_coverage,
#     peak_coverage
#   )
```

```{r}
# evidence %>% 
#   dplyr::filter(!is.na(number_of_data_points)) %>% 
#   as.data.frame() %>% 
#   dplyr::mutate(number_of_data_points = cut_number(number_of_data_points, 5)) %>% 
#   ggplot(aes(retention_time, number_of_data_points)) +
#   geom_boxplot()
# 
# evidence %>%
#   # dplyr::slice_sample(n=10000) %>% 
#   tidyr::drop_na(ms_ms_count) %>% 
#   dplyr::group_by(raw_file) %>% 
#   dplyr::count(ms_ms_count) %>% 
#   dplyr::mutate(prop = n/sum(n)) %>% 
#   dplyr::filter(charge == 2) %>% arrange(-prop) %>%
#   dplyr::ungroup() %>% 
#   dplyr::mutate(evidence__charge2_prop = log(n*prop)) %>% 
#   dplyr::select(raw_file, evidence__charge2_prop)
#   View()
# 
# evidence %>%
#   # dplyr::slice_sample(n=10000) %>% 
#   # tidyr::drop_na(number_of_scans) %>% 
#   dplyr::group_by(raw_file) %>% 
#   dplyr::summarise(evidence__number_of_scans = mean(number_of_scans, na.rm = TRUE)) %>% 
#   View()
# 
# evidence %>%
#   dplyr::slice_sample(n=10000) %>% 
#   tidyr::drop_na() %>% 
#   ggplot(aes(retention_time, number_of_scans)) +
#   geom_point() 
# 
# msms_scan %>% 
#   dplyr::slice_sample(n=1000) %>% 
#   # tidyr::drop_na() %>%
#   ggplot(aes(retention_time, log2(total_ion_current))) +
#   geom_point() 
# 
# ms_scan %>% 
#   dplyr::slice_sample(n=10000) %>% 
#   # tidyr::drop_na() %>%
#   dplyr::mutate(dplyr::across(c(total_ion_current, base_peak_intensity, raw_ov_ft_t), ~ log2(.x))) %>% 
#   View()
# 
# msms_scan %>% 
#   dplyr::slice_sample(n=1000) %>% View()
```


## calculate median form data

```{r calculate-median}

# evidence_median <-
#   evidence %>%
#   dplyr::select(-c(
#     charge,
#     retention_time,
#     ms_ms_count
#   )) %>%
#   dplyr::mutate(intensity = log2(intensity)) %>%
#   dplyr::group_by(raw_file, experiment) %>%
#   dplyr::summarise(dplyr::across(dplyr::everything(), ~ median(.x, na.rm = TRUE), .names = "evidence__{.col}")) %>%
#   dplyr::ungroup() %>%
#   ## new feature engineering
#   dplyr::left_join(
#     evidence %>%
#       tidyr::drop_na(charge) %>%
#       dplyr::group_by(raw_file) %>%
#       dplyr::count(charge) %>%
#       dplyr::mutate(prop = n / sum(n)) %>%
#       dplyr::filter(charge == 2) %>%
#       dplyr::ungroup() %>%
#       dplyr::mutate(evidence__charge2_prop = log(n * prop)) %>%
#       dplyr::select(raw_file, evidence__charge2_prop),
#     by = "raw_file"
#   ) %>% 
#   dplyr::left_join(
#     evidence %>%
#       tidyr::drop_na(ms_ms_count) %>%
#       dplyr::group_by(raw_file) %>%
#       dplyr::count(ms_ms_count) %>%
#       dplyr::mutate(prop = n / sum(n)) %>%
#       dplyr::filter(ms_ms_count == 1) %>% 
#       dplyr::ungroup() %>%
#       dplyr::mutate(evidence__ms_ms_count1_prop = log(n * prop)) %>%
#       dplyr::select(raw_file, evidence__ms_ms_count1_prop),
#     by = "raw_file"
#   )
# 
# # all_peptides_median <- 
# #   all_peptides %>% 
# #   dplyr::group_by(raw_file) %>% 
# #   dplyr::summarise(dplyr::across(dplyr::everything(), ~ median(.x, na.rm = TRUE), .names = "all_peptides__{.col}")) %>% 
# #   dplyr::ungroup()
# 
# ms_scan_median <- 
#   ms_scan %>% 
#   dplyr::select(-retention_time) %>% 
#   dplyr::mutate(dplyr::across(c(total_ion_current, base_peak_intensity, raw_ov_ft_t), ~ log2(.x))) %>%
#   dplyr::group_by(raw_file) %>% 
#   dplyr::summarise(dplyr::across(dplyr::everything(), ~ median(.x, na.rm = TRUE), .names = "ms_scan__{.col}")) %>% 
#   dplyr::ungroup()
# 
# msms_scan_median <- 
#   msms_scan %>% 
#   dplyr::select(-c(retention_time, elapsed_time, identified)) %>% 
#   dplyr::mutate(dplyr::across(c(total_ion_current, base_peak_intensity, precursor_intensity), ~ log2(.x))) %>%
#   dplyr::group_by(raw_file) %>% 
#   dplyr::summarise(dplyr::across(dplyr::everything(), ~ median(.x, na.rm = TRUE), .names = "msms_scan__{.col}")) %>% 
#   dplyr::ungroup()
# 
# # msms_median <- 
# #   msms %>% 
# #   dplyr::select(-c(retention_time, mass_deviations_ppm)) %>% 
# #   dplyr::group_by(raw_file) %>% 
# #   dplyr::summarise(dplyr::across(dplyr::everything(), ~ median(.x, na.rm = TRUE), .names = "msms__{.col}")) %>% 
# #   dplyr::ungroup()
# ```
# 
# ## prepare pg
# 
# ```{r PG}
# protein_groups_counts <- 
#   protein_groups %>% 
#   tidyr::pivot_longer(starts_with("peptides_"), names_to = "experiment", values_to = "peptides") %>%
#   dplyr::mutate(experiment = stringr::str_remove(string = experiment, pattern = "peptides_")) %>% 
#   dplyr::mutate(experiment = stringr::str_to_upper(string = experiment)) %>% 
#   dplyr::group_by(experiment) %>% 
#   dplyr::filter(peptides > 1) %>% 
#   dplyr::count(name = "protein_counts") %>% 
#   dplyr::ungroup()
```

## marge all tables

```{r merge}
# master_table <- 
#   summary %>% 
#   dplyr::left_join(evidence_median, by = "raw_file") %>% 
#   # dplyr::left_join(all_peptides_median, by = "raw_file") %>% 
#   dplyr::left_join(ms_scan_median, by = "raw_file") %>% 
#   dplyr::left_join(msms_scan_median, by = "raw_file") %>% 
#   # dplyr::left_join(msms_median, by = "raw_file") %>% 
#   dplyr::left_join(protein_groups_counts, by = "experiment") %>% 
#   dplyr::relocate(experiment, .after = raw_file) %>% 
#   dplyr::filter(!raw_file == "HF191128_QC_01" & !raw_file == "HF191128_QC_02" & !raw_file == "HF191128_QC_03") %>%
#   # dplyr::mutate(performance = if_else(protein_counts >= 2300, "good", "bad")) %>%
#   dplyr::mutate(performance = dplyr::case_when(ms_ms_identified_percent >= 45 ~ "good",
#                                                ms_ms_identified_percent < 40 ~ "bad",
#                                                TRUE ~ "warning")) %>%
#   dplyr::filter(!is.na(performance)) %>% 
#   dplyr::mutate(instrument = dplyr::if_else(stringr::str_detect(string = raw_file, pattern = "HF"), "hf", "qep")) %>% 
#   dplyr::select(-raw_file) %>%
#   dplyr::mutate(dplyr::across(dplyr::where(is.numeric), as.double)) %>% 
#   dplyr::mutate(instrument = as.factor(instrument)) %>% 
#   dplyr::mutate(performance = factor(performance, levels = c("bad", "warning", "good"))) %>% 
#   dplyr::mutate(ms_ms_identified_percent = as.factor(ms_ms_identified_percent)) %>% 
#   tidyr::drop_na() %>% 
#   dplyr::relocate(c(instrument, ms_ms_identified_percent), .after = experiment)
# 
# write.table(
#   x = master_table,
#   file = "C:/Users/ieo4973/Documents/QC_model/master_table.txt",
#   quote = FALSE,
#   sep = "\t",
#   row.names = FALSE,
#   col.names = TRUE
# )

```


## Reload master table

```{r}
master_table <- 
  data.table::fread(input = "C:/Users/ieo4973/Documents/QC_model/master_table.txt") %>% 
  dplyr::mutate(instrument = as.factor(instrument)) %>% 
  dplyr::mutate(performance = factor(performance, levels = c("good", "warning", "bad"))) %>% 
  dplyr::mutate(ms_ms_identified_percent = as.factor(ms_ms_identified_percent)) %>% 
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), as.double))
```

## Outcome distribution
```{r}
master_table %>% 
  dplyr::count(performance)
```

## EDA base

```{r eda}
master_table %>%
  tidyr::pivot_longer(
    -c(experiment, performance, instrument, ms_ms_identified_percent),
    names_to = "feature",
    values_to = "value"
  ) %>%
  ggplot2::ggplot(aes(x = value + 1, y = feature, fill = performance)) +
  geom_boxplot() +
  scale_fill_manual(values = list("good" = "darkgreen", "warning" = "orange", "bad" = "red")) +
  scale_x_log10() +
  theme_bw()
```

## data splitting e resampling

```{r splitting}
set.seed(123)

splits <- rsample::initial_split(master_table, prop = 0.8, strata = performance)

train <- rsample::training(splits)

test <- rsample::testing(splits)
```

## dara resamplig

```{r resampling}
set.seed(123)

folds <- rsample::vfold_cv(train, strata = performance)
```

## Create the recipes

```{r recipes}
base_recipie <- recipes::recipe(performance ~ ., data = train) %>% 
  recipes::update_role(c(experiment, ms_ms_identified_percent, instrument), new_role = "ID") %>% 
  recipes::step_zv(recipes::all_predictors()) %>% 
  recipes::step_normalize(recipes::all_predictors())

corr_recipie <- base_recipie %>% 
  recipes::step_corr(recipes::all_numeric_predictors())

feature_sel_model <- rand_forest(mode = "classification") %>%
    set_engine("ranger", importance = "permutation")

#feature importance
fs_recipie <- base_recipie %>% 
  recipeselectors::step_select_vip(
    recipes::all_predictors(),
    outcome = "performance",
    model = feature_sel_model,
    top_p = 25,
    threshold = 0.5
  )

pca_recipie <- base_recipie %>% 
  recipes::step_pca(recipes::all_predictors(), num_comp = 10)
```

## prep recipes and EDA

```{r bake}
base_juice <- base_recipie %>% 
  prep() %>% 
  juice()

corr_juice <- corr_recipie %>% 
  prep() %>% 
  juice()

fs_juice <- fs_recipie %>% 
  prep() %>% 
  juice()

pca_juice <- pca_recipie %>% 
  prep() %>% 
  juice()

base_juice %>%
  tidyr::pivot_longer(
    -c(experiment, performance, ms_ms_identified_percent, instrument),
    names_to = "feature",
    values_to = "value"
  ) %>% 
  ggplot2::ggplot(aes(x = value +1, y = feature, fill = performance)) +
  geom_boxplot() +
  scale_fill_manual(values = list("good" = "darkgreen", "warning" = "orange", "bad" = "red")) +
  scale_x_log10() +
  theme_bw()

## scartati dal filtro della correlazione
corr_recipie %>%
  prep() %>%
  tidy(3)

## scartati dal feature selection moldel
fs_recipie %>%
  prep() %>%
  tidy(3)

## correlation matrix
base_juice %>% 
  corrr::correlate() %>% 
  corrr::shave(upper = TRUE) %>%
  corrr::rplot() +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

base_juice %>% 
  corrr::correlate() %>% 
  corrr::focus(protein_counts) %>% View()

## pca

p1 <- pca_juice %>% 
  ggplot(aes(PC01, PC02, color = performance)) +
  geom_point()+
  scale_color_manual(values = list("good" = "darkgreen", "warning" = "orange", "bad" = "red")) +
  theme_bw()

p1

p2 <- pca_recipie %>% 
  prep() %>% 
  tidy(3) %>% 
  filter(component %in% paste0("PC", 1:5)) %>%
  mutate(component = forcats::fct_inorder(component)) %>%
  ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~component, nrow = 1) +
  labs(y = NULL)
p2

patchwork::wrap_plots(p1,p2, ncol = 1)

## explore
base_juice %>% 
  dplyr::select(-c(experiment, ms_ms_identified_percent)) %>% 
  explore()
```


## models

```{r models}
log_reg <- parsnip::logistic_reg(
  mode = "classification",
  engine = "LiblineaR",
  penalty = tune::tune(),
  mixture = 1
)

multi_reg <- parsnip::multinom_reg(
  mode = "classification",
  engine = "glmnet",
  penalty = tune::tune(),
  mixture = 1
)

xgboost <- parsnip::boost_tree(
  mode = "classification",
  engine = "xgboost",
  mtry = tune::tune(),
  trees = tune::tune(),
  min_n = tune::tune(),
  tree_depth = tune::tune(),
  learn_rate = tune::tune()
)

svm_lin <- parsnip::svm_linear(
  mode = "classification",
  engine = "LiblineaR",
  cost = tune::tune()
)

svm_nlin <- parsnip::svm_rbf(
  mode = "classification",
  engine = "kernlab",
  cost = tune::tune()
)
```

## logistic reg workflow

```{r log-reg-workflow}
log_reg_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(log_reg) %>% 
  workflows::add_recipe(base_recipie)
```

### tuning hyperparameters

```{r log-reg-tuning}
set.seed(123)
log_reg_grid <- dials::grid_regular(dials::penalty(range = c(-5, 0)), levels = 20)
```

### train and tune the model

```{r log-reg-train-tune}
doParallel::registerDoParallel()
set.seed(123)

log_reg_res <- log_reg_wflow %>%
  tune::tune_grid(
    resamples =folds,
    grid = log_reg_grid,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)
  )


log_reg_res %>% collect_metrics()
# 
# select_best(log_reg_res)

final_wf_log_reg <- finalize_workflow(log_reg_wflow, select_best(log_reg_res))

final_log_reg <- last_fit(
  final_wf_log_reg,
  split = splits,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)
) 

final_log_reg %>% collect_metrics() 

final_log_reg %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 

final_log_reg %>%
  collect_predictions() %>% 
  roc_curve(performance, c(.pred_good, .pred_warning, .pred_bad)) %>% 
  ggplot(aes(1- specificity, sensitivity, color = .level)) +
  geom_abline(slope = 1, color = "gray50", lty = 2, alpha = 0.8) +
  geom_path(linewidth = 1.5, alpha = 0.7) +
  labs(color = NULL) +
  theme_bw() +
  coord_fixed()

```

### feature importance

```{r feature-importance}
final_log_reg %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  dplyr::mutate(estimate = exp(estimate)) %>%
  dplyr::filter(!term == "Bias") %>% 
  dplyr::mutate(term = forcats::fct_reorder(term, estimate)) %>% 
  ggplot(aes(estimate, term)) +
  geom_col()
```

## svm linear workflow

```{r svm_lin-workflow}
svm_lin_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(svm_lin) %>% 
  workflows::add_recipe(base_recipie)

```

### tuning hyperparameters

```{r svm_lin-tuning}
set.seed(123)
svm_lin_grid <- dials::grid_max_entropy(dials::cost(), size = 20)
```

### train and tune the model

```{r svm_lin-train-tune}
doParallel::registerDoParallel()
set.seed(123)

svm_lin_res <- svm_lin_wflow %>%
  tune::tune_grid(
    resamples =folds,
    grid = svm_lin_grid,
    metrics = metric_set(accuracy)
  )

# svm_lin_res %>%
#   collect_metrics()
# 
# select_best(svm_lin_res)

svm_lin_final_wf <- finalize_workflow(svm_lin_wflow, select_best(svm_lin_res))

final_svm_lin <- last_fit(svm_lin_final_wf,
                          split = splits,
                          metrics = metric_set(accuracy)) 

final_svm_lin %>% collect_metrics()

final_svm_lin %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 
```

### feature importance

```{r feature-importance}
final_svm_lin %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  dplyr::mutate(estimate = abs(estimate)) %>%
  dplyr::filter(!term == "Bias") %>% 
  dplyr::mutate(term = forcats::fct_reorder(term, estimate)) %>% 
  ggplot(aes(estimate, term)) +
  geom_col()
```

## svm non-linear workflow

```{r svm_lin-workflow}
svm_nlin_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(svm_nlin) %>% 
  workflows::add_recipe(base_recipie)
```

### tuning hyperparameters

```{r svm_lin-tuning}
set.seed(123)
svm_nlin_grid <- dials::grid_max_entropy(dials::cost(), size = 20)
```

### train and tune the model

```{r svm_lin-train-tune}
doParallel::registerDoParallel()
set.seed(123)

svm_nlin_res <- svm_nlin_wflow %>%
  tune::tune_grid(
    resamples =folds,
    grid = svm_nlin_grid,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)
  )


# svm_nlin_res %>% collect_metrics()
# 
# select_best(svm_nlin_res)

svm_nlin_final_wf <- finalize_workflow(svm_nlin_wflow, select_best(svm_nlin_res))

final_svm_nlin <- last_fit(svm_nlin_final_wf,
                          split = splits,
                          metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)) 

final_svm_nlin %>% collect_metrics()

final_svm_nlin %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 


final_svm_nlin %>%
  collect_predictions() %>% 
  roc_curve(performance, .pred_good:.pred_bad) %>% 
  ggplot(aes(1- specificity, sensitivity, color = .level)) +
  geom_abline(slope = 1, color = "gray50", lty = 2, alpha = 0.8) +
  geom_path(linewidth = 1.5, alpha = 0.7) +
  labs(color = NULL) +
  theme_bw() +
  coord_fixed()
```

### feature importance

```{r}
set.seed(123)

final_svm_nlin %>%
  extract_workflow() %>%
  pull_workflow_fit() %>%
  vi(
    method = "permute",
    metric = "auc",
    nsim = 10,
    target = "performance",
    reference_class = "good", #cosa spinge la classificazione ad essere definita good
    pred_wrapper = kernlab::predict,
    train = base_juice
  ) %>%
  dplyr::filter(!Variable %in% c("instrument", "experiment", "ms_ms_identified_percent")) %>%
  # dplyr::mutate(Sign = if_else(Importance >= 0, "POS", "NEG")) %>%
  # dplyr::mutate(Importance = abs(Importance)) %>%
  dplyr::mutate(Variable = forcats::fct_reorder(Variable, Importance)) %>%
  ggplot(aes(Importance, Variable)) +
  geom_errorbar(aes(xmin = Importance- StDev, xmax = Importance + StDev)) +
  geom_col()
```


## xgboost workflow

```{r svm_lin-workflow}
xgboost_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(xgboost) %>% 
  workflows::add_recipe(base_recipie)
```

### tuning hyperparameters

```{r svm_lin-tuning}
set.seed(123)
xgboost_grid <- dials::grid_max_entropy(dials::finalize(dials::mtry(), train),
                                        dials::trees(),
                                        dials::min_n(),
                                        dials::tree_depth(),
                                        dials::learn_rate(),
                                        size = 20)
```

### train and tune the model

```{r svm_lin-train-tune}
doParallel::registerDoParallel()
set.seed(123)

xgboost_res <- xgboost_wflow %>%
  tune::tune_grid(
    resamples =folds,
    grid = xgboost_grid,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)
  )


# xgboost_res %>% collect_metrics()
# 
# select_best(xgboost_res)

xgboost_final_wf <- finalize_workflow(xgboost_wflow, select_best(xgboost_res))

final_xgboost <- last_fit(xgboost_final_wf,
                          split = splits,
                          metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)) 

final_xgboost %>% collect_metrics()

final_xgboost %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 

final_xgboost %>%
  collect_predictions() %>% 
  roc_curve(performance, .pred_good:.pred_bad) %>% 
  ggplot(aes(1- specificity, sensitivity, color = .level)) +
  geom_abline(slope = 1, color = "gray50", lty = 2, alpha = 0.8) +
  geom_path(linewidth = 1.5, alpha = 0.7) +
  labs(color = NULL) +
  theme_bw() +
  coord_fixed()

```

### feature importance

```{r}
final_xgboost %>% 
  extract_fit_parsnip() %>% 
  vip()
```


## multi logistic reg workflow

```{r log-reg-workflow}
multi_reg_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(multi_reg) %>% 
  workflows::add_recipe(base_recipie)

```

### tuning hyperparameters

```{r log-reg-tuning}
multi_grid <- dials::grid_regular(dials::penalty(range = c(-5, 0)), levels = 20)
```

### train and tune the model

```{r log-reg-train-tune}
doParallel::registerDoParallel()
set.seed(123)

multi_reg_res <- multi_reg_wflow %>%
  tune::tune_grid(
    resamples = folds,
    grid = multi_grid,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)
  )

autoplot(multi_reg_res)
# log_reg_res %>%
#   collect_metrics()
# 
# select_best(log_reg_res)

sd_penal <- select_by_one_std_err(multi_reg_res, metric = "roc_auc", desc(penalty))

multi_final_wf <- finalize_workflow(multi_reg_wflow, sd_penal)

final_multi <- last_fit(multi_final_wf, split = splits, metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)) 

final_multi %>% collect_metrics()

final_multi %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 

final_multi %>% 
  collect_predictions() %>% 
  roc_curve(performance, c(.pred_good, .pred_warning, .pred_bad)) %>% 
  ggplot(aes(1- specificity, sensitivity, color = .level)) +
  geom_abline(slope = 1, color = "gray50", lty = 2, alpha = 0.8) +
  geom_path(linewidth = 1.5, alpha = 0.7) +
  labs(color = NULL) +
  theme_bw() +
  coord_fixed()

```

### feature importance

```{r}
final_multi %>% 
  extract_fit_parsnip() %>% 
  vip()
```


## workflow set

```{r workflow-set}
compare_models <- workflow_set(
  preproc = list(
    base = base_recipie,
    corr = corr_recipie
    # fs = fs_recipie
  ),
  models = list(
    log_reg = log_reg,
    multi_reg = multi_reg,
    tree = xgboost,
    # svm_lin = svm_lin,
    svm_nlin = svm_nlin
  ),
  cross = TRUE
)
```

## tuning all

```{r tuning-all}
doParallel::registerDoParallel()
set.seed(123)

data_frame_grid <- 
  bind_cols(log_reg_grid, svm_lin_grid, xgboost_grid)

all_models <- 
  compare_models %>% 
  workflow_map(
    fn = "tune_race_anova",
    resamples = folds,
    grid = 20,
    # control = control_grid(save_pred = TRUE),
    control = control_race(save_pred = TRUE),
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
    verbose = TRUE
  )
```

## compare models

```{r}
autoplot(
  all_models,
  rank_metric = "roc_auc",
  metric = "roc_auc",
  select_best = TRUE
) +
  geom_text(aes(y = mean - 0.02, label = wflow_id),
            angle = 90,
            hjust = 1) +
  lims(y = c(0.92, 1)) +
  theme(legend.position = "none") +
  theme_bw()
```

```{r}
rank_results(all_models)
```

## finalizing model

```{r}
best_result <-
  all_models %>%
  extract_workflow_set_result("base_multi_reg") %>%
  select_best(metric = "roc_auc")

best_base_multi_reg <-
  all_models %>%
  extract_workflow("base_multi_reg") %>%
  finalize_workflow(best_result) %>%
  last_fit(split = splits)

```

```{r}

best_base_multi_reg %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 

best_base_multi_reg %>% collect_metrics()

best_base_multi_reg %>% 
  collect_predictions() %>% 
  roc_curve(performance, c(.pred_good, .pred_warning, .pred_bad)) %>% 
  ggplot(aes(1- specificity, sensitivity, color = .level)) +
  geom_abline(slope = 1, color = "gray50", lty = 2, alpha = 0.8) +
  geom_path(linewidth = 1.5, alpha = 0.7) +
  labs(color = NULL) +
  theme_bw() +
  coord_fixed()

best_base_multi_reg %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 15)

saveRDS(best_base_multi_reg %>% extract_workflow(), file = "C:/Users/ieo4973/Documents/QC_model/best_base_multi_reg.rds")

model <- readRDS(file = "C:/Users/ieo4973/Documents/QC_model/best_base_multi_reg.rds")

model %>% 
  augment(test) %>% 
  View()
```


---
title: "median_model"
format: html
editor: visual
---

## load libraries

```{r libraries}
library(data.table)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(explore)
```

## path to files

```{r path-to-files}
# path_files <- "X:/PublicData/QC/HF/2022/02/txt/"
path_files <- "X:/PublicData/QC/temporary_all_HF/QC_comb_txt_results/"
```


## load files

```{r files}
summary <-
  data.table::fread(input = paste0(path_files, "summary.txt")) %>% 
  tibble::as_tibble(.name_repair = janitor::make_clean_names) %>% 
  dplyr::select(
    raw_file,
    ms_ms,
    ms_ms_submitted,
    ms_ms_identified,
    ms_ms_identified_percent,
    peptide_sequences_identified,
    peaks_sequenced_percent,
    peaks_repeatedly_sequenced_percent
  ) %>% 
  dplyr::filter(!raw_file == "Total")

evidence <-
  data.table::fread(input = paste0(path_files, "evidence.txt")) %>%
  tibble::as_tibble(.name_repair = janitor::make_clean_names) %>% 
  dplyr::filter(!reverse == "+" & !potential_contaminant == "+") %>%
  dplyr::select(
    raw_file,
    experiment,
    charge,
    m_z,
    mass,
    score,
    uncalibrated_calibrated_m_z_ppm,
    mass_error_ppm,
    uncalibrated_mass_error_ppm,
    max_intensity_m_z_0,
    retention_time,
    retention_length,
    number_of_data_points,
    number_of_scans,
    ms_ms_count,
    intensity
  ) 

protein_groups <-
  data.table::fread(input = paste0(path_files, "proteinGroups.txt")) %>%
  tibble::as_tibble(.name_repair = janitor::make_clean_names) %>%
  dplyr::filter(!reverse == "+" & !potential_contaminant == "+" & !only_identified_by_site == "+") %>% 
  dplyr::select(starts_with("peptides_"))

# all_peptides <-
#   data.table::fread(input = paste0(path_files, "allPeptides.txt")) %>%
#   tibble::as_tibble(.name_repair = janitor::make_clean_names) %>%
#   dplyr::select(
#     raw_file,
#     mass_precision_ppm,
#     retention_time,
#     retention_length,
#     retention_length_fwhm
#     # score
#     # msms_isotope_indices
#   )

ms_scan <-
  data.table::fread(input = paste0(path_files, "msScans.txt")) %>%
  tibble::as_tibble(.name_repair = janitor::make_clean_names) %>% 
  dplyr::select(
    raw_file,
    retention_time,
    cycle_time,
    ion_injection_time,
    total_ion_current,
    base_peak_intensity,
    peak_length,
    identified_multiplets_s,
    ms_ms_s,
    identified_ms_ms_s,
    ms_ms_identification_rate_percent,
    intens_comp_factor,
    ctcd_comp,
    raw_ov_ft_t,
    agc_fill
  ) 

msms_scan <-
  data.table::fread(input = paste0(path_files, "msmsScans.txt")) %>% 
  tibble::as_tibble(.name_repair = janitor::make_clean_names) %>%
  dplyr::select(
    raw_file,
    retention_time,
    ion_injection_time,
    total_ion_current,
    collision_energy,
    base_peak_intensity,
    elapsed_time,
    identified,
    precursor_intensity,
    score
  ) 

# msms <-
#   data.table::fread(input = paste0(path_files, "msms.txt")) %>%
#   tibble::as_tibble(.name_repair = janitor::make_clean_names) %>% 
#   dplyr::select(
#     raw_file, 
#     isotope_index,
#     mass_error_ppm,
#     simple_mass_error_ppm,
#     precursor_intensity,
#     retention_time,
#     score,
#     mass_deviations_ppm,
#     number_of_matches,
#     intensity_coverage,
#     peak_coverage
#   )
```

```{r}
evidence %>% 
  dplyr::count(number_of_data_points) %>% 
  # dplyr::filter(n > 1) %>% 
  tidyr::drop_na() %>% 
  ggplot(aes(number_of_data_points, log2(n))) +
  geom_col()
```


## calculate median form data

```{r calculate-median}

evidence_median <- 
  evidence %>% 
  dplyr::select(-c(charge, retention_time, number_of_data_points, number_of_scans, ms_ms_count)) %>% 
  dplyr::group_by(raw_file, experiment) %>% 
  dplyr::summarise(dplyr::across(dplyr::everything(), ~ median(.x, na.rm = TRUE), .names = "evidence__{.col}")) %>% 
  dplyr::ungroup()

# all_peptides_median <- 
#   all_peptides %>% 
#   dplyr::group_by(raw_file) %>% 
#   dplyr::summarise(dplyr::across(dplyr::everything(), ~ median(.x, na.rm = TRUE), .names = "all_peptides__{.col}")) %>% 
#   dplyr::ungroup()

ms_scan_median <- 
  ms_scan %>% 
  dplyr::select(-retention_time) %>% 
  dplyr::group_by(raw_file) %>% 
  dplyr::summarise(dplyr::across(dplyr::everything(), ~ median(.x, na.rm = TRUE), .names = "ms_scan__{.col}")) %>% 
  dplyr::ungroup()

msms_scan_median <- 
  msms_scan %>% 
  dplyr::select(-c(retention_time, elapsed_time, identified)) %>% 
  dplyr::group_by(raw_file) %>% 
  dplyr::summarise(dplyr::across(dplyr::everything(), ~ median(.x, na.rm = TRUE), .names = "msms_scan__{.col}")) %>% 
  dplyr::ungroup()

# msms_median <- 
#   msms %>% 
#   dplyr::select(-c(retention_time, mass_deviations_ppm)) %>% 
#   dplyr::group_by(raw_file) %>% 
#   dplyr::summarise(dplyr::across(dplyr::everything(), ~ median(.x, na.rm = TRUE), .names = "msms__{.col}")) %>% 
#   dplyr::ungroup()
```

## prepare pg

```{r PG}
protein_groups_counts <- 
  protein_groups %>% 
  tidyr::pivot_longer(starts_with("peptides_"), names_to = "experiment", values_to = "peptides") %>%
  dplyr::mutate(experiment = stringr::str_remove(string = experiment, pattern = "peptides_")) %>% 
  dplyr::mutate(experiment = stringr::str_to_upper(string = experiment)) %>% 
  dplyr::group_by(experiment) %>% 
  dplyr::filter(peptides > 1) %>% 
  dplyr::count(name = "protein_counts") %>% 
  dplyr::ungroup()
```

## marge all tables

```{r merge}
master_table <- 
  summary %>% 
  dplyr::left_join(evidence_median, by = "raw_file") %>% 
  # dplyr::left_join(all_peptides_median, by = "raw_file") %>% 
  dplyr::left_join(ms_scan_median, by = "raw_file") %>% 
  dplyr::left_join(msms_scan_median, by = "raw_file") %>% 
  # dplyr::left_join(msms_median, by = "raw_file") %>% 
  dplyr::left_join(protein_groups_counts, by = "experiment") %>% 
  dplyr::relocate(experiment, .after = raw_file) %>% 
  dplyr::filter(!raw_file == "HF191128_QC_01" & !raw_file == "HF191128_QC_02" & !raw_file == "HF191128_QC_03") %>%
  # dplyr::mutate(performance = if_else(protein_counts >= 2300, "good", "bad")) %>%
  dplyr::mutate(performance = dplyr::case_when(ms_ms_identified_percent >= 45 ~ "good",
                                               ms_ms_identified_percent < 40 ~ "bad",
                                               TRUE ~ "warning")) %>%
  dplyr::filter(!is.na(performance)) %>% 
  dplyr::mutate(instrument = dplyr::if_else(stringr::str_detect(string = raw_file, pattern = "HF"), "hf", "qep")) %>% 
  dplyr::select(-raw_file) %>%
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), as.double)) %>% 
  dplyr::mutate(instrument = as.factor(instrument)) %>% 
  dplyr::mutate(performance = factor(performance, levels = c("bad", "warning", "good"))) %>% 
  dplyr::mutate(ms_ms_identified_percent = as.factor(ms_ms_identified_percent)) %>% 
  tidyr::drop_na()

```

## EDA

```{r eda}
master_table %>% 
  tidyr::pivot_longer(-c(experiment, performance, instrument), names_to = "feature", values_to = "value") %>% 
  ggplot2::ggplot(aes(x = value +1, y = feature, fill = performance)) +
  geom_boxplot() +
  scale_x_log10()
```


## model
```{r}
master_table %>% 
  dplyr::count(performance)
```

## data splitting e resampling

```{r splitting}
set.seed(123)

splits <- rsample::initial_split(master_table, strata = performance)

train <- rsample::training(splits)

test <- rsample::testing(splits)
```

## dara resamplig

```{r resampling}
set.seed(123)

folds <- rsample::vfold_cv(train, strata = performance)
```

## models

```{r models}
log_reg <- parsnip::logistic_reg(
  mode = "classification",
  engine = "LiblineaR",
  penalty = tune::tune(),
  mixture = 1
)

xgboost <- parsnip::boost_tree(
  mode = "classification",
  engine = "xgboost",
  mtry = tune::tune(),
  trees = tune::tune(),
  min_n = tune::tune(),
  tree_depth = tune::tune(),
  learn_rate = tune::tune()
)

svm_lin <- parsnip::svm_linear(
  mode = "classification",
  engine = "LiblineaR",
  cost = tune::tune()
)

svm_nlin <- parsnip::svm_rbf(
  mode = "classification",
  engine = "kernlab",
  cost = tune::tune()
)

multi_reg <- parsnip::multinom_reg(
  mode = "classification",
  engine = "glmnet",
  penalty = tune::tune(),
  mixture = 1
)
```

## Create the recipes

```{r recipes}
feature_sel_model <- rand_forest(mode = "classification") %>%
    set_engine("ranger", importance = "permutation")

very_base_racipie <- recipes::recipe(performance ~ ., data = train) %>% 
  recipes::update_role(c(experiment, ms_ms_identified_percent, instrument), new_role = "ID") %>% 
  recipes::step_zv(recipes::all_predictors()) %>% 
  recipes::step_normalize(recipes::all_predictors())

base_recipie <- recipes::recipe(performance ~ ., data = train) %>% 
  recipes::update_role(c(experiment, ms_ms_identified_percent, instrument), new_role = "ID") %>% 
  recipes::step_zv(recipes::all_predictors()) %>% 
  recipes::step_normalize(recipes::all_predictors()) %>% 
  recipes::step_corr(recipes::all_numeric_predictors()) %>%
  # recipeselectors::step_select_vip(
  #   recipes::all_predictors(),
  #   outcome = "performance",
  #   model = feature_sel_model,
  #   top_p = 15,
  #   threshold = 0.75
  # )

pca_recipie <- base_recipie %>% 
  recipes::step_pca(recipes::all_predictors(), num_comp = 10)
  # embed::step_umap(recipes::all_predictors())
```

## bake recipes and EDA

```{r bake}
vbase_juice <- very_base_racipie %>% 
  prep() %>% 
  juice()

base_recipie %>%  prep() %>%  tidy(4)


vbase_juice %>% 
  corrr::correlate() %>% 
  # corrr::network_plot(min_cor = .2)
  corrr::shave(upper = TRUE) %>%
  corrr::rplot() +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

vbase_juice %>% 
  corrr::correlate() %>% 
  corrr::focus(protein_counts) %>% View()

base_juice <- base_recipie %>% 
  prep() %>% 
  juice()

pca_juice <- pca_recipie %>% 
  prep() %>% 
  juice()

base_juice %>% 
  tidyr::pivot_longer(-c(experiment, performance, ms_ms_identified_percent, instrument), names_to = "feature", values_to = "value") %>% 
  ggplot2::ggplot(aes(x = value +1, y = feature, fill = performance)) +
  geom_boxplot() +
  scale_x_log10()

base_juice %>% 
  dplyr::select(-c(experiment, ms_ms_identified_percent)) %>% 
  explore()
  explain_tree(target = performance)


pca_juice
  ggplot(aes(PC01, PC02, color = performance)) +
  geom_point()

p2 <- pca_recipie  %>% 
  prep() %>% 
  tidy(6) %>% 
  filter(component %in% paste0("PC", 1:5)) %>%
  mutate(component = forcats::fct_inorder(component)) %>%
  ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~component, nrow = 1) +
  labs(y = NULL)

patchwork::wrap_plots(p1,p2, ncol = 1)

base_juice %>% 
  ggplot(aes(evidence__score, performance)) +
  geom_point()
```

## logistic reg workflow

```{r log-reg-workflow}
log_reg_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(log_reg) %>% 
  workflows::add_recipe(base_recipie)

```

### tuning hyperparameters

```{r log-reg-tuning}
set.seed(123)
log_reg_grid <- dials::grid_regular(dials::penalty(), levels = 5)
```

### train and tune the model

```{r log-reg-train-tune}
doParallel::registerDoParallel()
set.seed(123)

log_reg_res <- log_reg_wflow %>%
  tune::tune_grid(
    resamples =folds,
    grid = log_reg_grid,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc)
  )


# log_reg_res %>% 
#   collect_metrics()
# 
# select_best(log_reg_res)

final_wf <- finalize_workflow(log_reg_wflow, select_best(log_reg_res))

final_log_reg <- last_fit(final_wf, split = splits) 

# final_log_reg %>% collect_metrics()

final_log_reg %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 

```

### feature importance

```{r feature-importance}
final_log_reg %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  dplyr::mutate(estimate = exp(estimate)) %>%
  arrange(desc(estimate))
```

```{r}
library(vip)

final_log_reg %>% 
  extract_fit_parsnip() %>% 
  vip()
```


## svm linear workflow

```{r svm_lin-workflow}
svm_lin_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(svm_lin) %>% 
  workflows::add_recipe(base_recipie)

```

### tuning hyperparameters

```{r svm_lin-tuning}
set.seed(123)
svm_lin_grid <- dials::grid_max_entropy(dials::cost(), size = 5)
```

### train and tune the model

```{r svm_lin-train-tune}

doParallel::registerDoParallel()
set.seed(123)

svm_lin_res <- svm_lin_wflow %>%
  tune::tune_grid(
    resamples =folds,
    grid = svm_lin_grid,
    metrics = metric_set(accuracy)
  )


svm_lin_res %>%
  collect_metrics()

select_best(svm_lin_res)

svm_lin_final_wf <- finalize_workflow(svm_lin_wflow, select_best(svm_lin_res))

final_svm_lin <- last_fit(svm_lin_final_wf,
                          split = splits,
                          metrics = metric_set(accuracy)) 

# final_log_reg %>% collect_metrics()

final_svm_lin %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 


final_svm_lin %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  dplyr::mutate(estimate = abs(estimate)) %>%
  dplyr::filter(!term == "Bias") %>% 
  dplyr::mutate(term = forcats::fct_reorder(term, estimate)) %>% 
  ggplot(aes(estimate, term)) +
  geom_col()
```


## svm non-linear workflow

```{r svm_lin-workflow}
svm_nlin_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(svm_nlin) %>% 
  workflows::add_recipe(base_recipie)
```

### tuning hyperparameters

```{r svm_lin-tuning}
set.seed(123)
svm_nlin_grid <- dials::grid_max_entropy(dials::cost(), size = 5)
```

### train and tune the model

```{r svm_lin-train-tune}
doParallel::registerDoParallel()
set.seed(123)

svm_nlin_res <- svm_nlin_wflow %>%
  tune::tune_grid(
    resamples =folds,
    grid = svm_nlin_grid,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)
  )


svm_nlin_res %>%
  collect_metrics()

select_best(svm_nlin_res)

svm_nlin_final_wf <- finalize_workflow(svm_nlin_wflow, select_best(svm_nlin_res))

final_svm_nlin <- last_fit(svm_nlin_final_wf,
                          split = splits,
                          metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)) 

# final_log_reg %>% collect_metrics()

final_svm_nlin %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 


set.seed(123)

final_svm_nlin %>% 
  extract_workflow() %>% 
  pull_workflow_fit() %>%
  vi(
    method = "permute",
    metric = "auc",  
    nsim = 10,
    target = "performance",
    reference_class = "good", #cosa spinge la classificazione ad essere definita good
    pred_wrapper = kernlab::predict,
    train = base_juice
  ) %>%
  dplyr::filter(!Variable %in% c("instrument", "experiment", "ms_ms_identified_percent")) %>% 
  # dplyr::mutate(Sign = if_else(Importance >= 0, "POS", "NEG")) %>% 
  # dplyr::mutate(Importance = abs(Importance)) %>% 
  dplyr::mutate(Variable = forcats::fct_reorder(Variable, Importance)) %>% 
  ggplot(aes(Importance, Variable)) +
  geom_errorbar(aes(xmin = Importance- StDev, xmax = Importance + StDev)) +
  geom_col()
```


## xgboost workflow

```{r svm_lin-workflow}
xgboost_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(xgboost) %>% 
  workflows::add_recipe(base_recipie)
```

### tuning hyperparameters

```{r svm_lin-tuning}
set.seed(123)
xgboost_grid <- dials::grid_max_entropy(dials::finalize(dials::mtry(), train),
                                        dials::trees(),
                                        dials::min_n(),
                                        dials::tree_depth(),
                                        dials::learn_rate(),
                                        size = 5)
```

### train and tune the model

```{r svm_lin-train-tune}
doParallel::registerDoParallel()
set.seed(123)

xgboost_res <- xgboost_wflow %>%
  tune::tune_grid(
    resamples =folds,
    grid = xgboost_grid,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)
  )


xgboost_res %>%
  collect_metrics()

select_best(xgboost_res)

xgboost_final_wf <- finalize_workflow(xgboost_wflow, select_best(xgboost_res))

final_xgboost <- last_fit(xgboost_final_wf,
                          split = splits,
                          metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)) 

# final_log_reg %>% collect_metrics()

final_xgboost %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 

final_xgboost %>% 
  collect_predictions() %>% 
  roc_curve(performance, .pred_bad:.pred_good) %>% 
  autoplot()


final_xgboost %>% 
  extract_fit_parsnip() %>% 
  vip()
```



## multi logistic reg workflow

```{r log-reg-workflow}
multi_reg_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(multi_reg) %>% 
  workflows::add_recipe(base_recipie)

```

### tuning hyperparameters

```{r log-reg-tuning}
multi_grid <- dials::grid_regular(dials::penalty(range = c(-5, 0)), levels = 20)
```

### train and tune the model

```{r log-reg-train-tune}
doParallel::registerDoParallel()
set.seed(123)

multi_reg_res <- multi_reg_wflow %>%
  tune::tune_grid(
    resamples = folds,
    grid = multi_grid,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)
  )

autoplot(multi_reg_res)
# log_reg_res %>%
#   collect_metrics()
# 
# select_best(log_reg_res)

sd_penal <- select_by_one_std_err(multi_reg_res, metric = "roc_auc", desc(penalty))

multi_final_wf <- finalize_workflow(multi_reg_wflow, sd_penal)

final_multi <- last_fit(multi_final_wf, split = splits) 

final_multi %>% collect_metrics()

final_multi %>% 
  collect_predictions() %>%  
  conf_mat(performance, .pred_class) %>% 
  autoplot(type = "heatmap") 

final_multi %>% 
  collect_predictions() %>% 
  roc_curve(performance, .pred_bad:.pred_good) %>% 
  ggplot(aes(1- specificity, sensitivity, color = .level)) +
  geom_abline(slope = 1, color = "gray50", lty = 2, alpha = 0.8) +
  geom_path(linewidth = 1.5, alpha = 0.7) +
  labs(color = NULL) +
  theme_bw() +
  coord_fixed()

final_multi %>% 
  extract_fit_parsnip() %>% 
  vip()
```
